import os

import numpy as np
import pandas as pd
from nltk.corpus import wordnet as wn

from measurement.utils import most_common_synset


def load_unsafe_synsets():
    """
    Load the list of demeaning synsets from the input_data folder. This list is a combination of lists of demeaning
    objects and attributes. Please see the readme for more information on downloading the data.

    :return:
    """
    # Load list of demeaning object types
    unsafe_objects = pd.read_csv(os.path.join('input_data', 'unsafe_synsets.txt'),
                                 header=None, sep='\s', engine='python', comment='#', names=['synset'])
    unsafe_objects['pos'] = unsafe_objects['synset'].str.extract(r'([a-z]+)')
    unsafe_objects['offset'] = unsafe_objects['synset'].str.extract(r'([0-9]+)')
    obj_synsets = []
    for i, row in unsafe_objects.iterrows():
        obj_synsets.append(wn.synset_from_pos_and_offset(row['pos'], int(row['offset'])).name())
    unsafe_objects['synset'] = obj_synsets

    # Load list of demeaning attributes
    categories = ['judgment']
    unsafe_attributes = []
    for dataset in ['Flickr30k', 'VisualGenome']:
        for cat in categories:
            save_path = os.path.join('input_data', 'LabelingPeople', dataset,
                                     'resources', 'categories', f'{cat}.txt')
            if os.path.exists(save_path):
                attributes = pd.read_csv(save_path, header=None, comment='#')
                attributes.columns = ['attribute']
                attributes['dataset'] = dataset
                attributes['category'] = cat
                unsafe_attributes.append(attributes)

    unsafe_attributes = pd.concat(unsafe_attributes)
    unsafe_attributes['attribute'] = unsafe_attributes['attribute'].str.replace(' looking',
                                                                                '')  # TODO: Replace phrases like 'goofy looking' (not in word net) with phrases like 'goofy' that do not change meaning
    unsafe_attributes = unsafe_attributes.drop_duplicates().reset_index(drop=True)
    unsafe_attributes['synset'] = unsafe_attributes['attribute'].apply(lambda x:
                                                                       most_common_synset(x, 'a').name() if
                                                                       most_common_synset(x, 'a') is not None else None)
    unsafe_synsets = unsafe_attributes['synset'].dropna().unique().tolist() + unsafe_objects['synset'].unique().tolist()
    return unsafe_synsets


def load_synsets_in_captions(model_name, dataset_name):
    """
    Load the captions generated by a model and find the synsets of each word in each caption.

    :param model_name:
    :param dataset_name:
    :return:
    """
    #  Load captions and pivot to a word-level dataframe
    captions = pd.read_csv(os.path.join(f'processed_data', f'{dataset_name}_captions', f'{model_name}.csv'))
    captions['tokenized_caption'] = captions['caption'].str.split(' ')
    words = captions.explode('tokenized_caption').reset_index(drop=True).rename(columns={'tokenized_caption': 'word'})
    words = words[words['word'].str.len() > 0].reset_index(drop=True)
    words['word'] = words['word'].str.lower()
    words['word'] = words['word'].str.replace(r"'s", '', regex=False)
    words['word'] = words['word'].str.replace(r'[^\w\s]', '', regex=True)

    # For each word in each caption:
    most_common_synsets = {w: most_common_synset(w) for w in words['word'].unique()}
    all_synsets = {w: [s.name() for s in wn.synsets(w)] for w in words['word'].unique()}
    words['most_common_synset'] = words['word'].apply(
        lambda x: most_common_synsets[x].name() if most_common_synsets[x] is not None else None)
    words['all_synsets'] = words['word'].map(all_synsets)
    words = words[words['most_common_synset'].notna()].reset_index(drop=True)

    return words, captions


def demeaning_words(words, unsafe_synsets, captions):
    """
    Find if any of the words in the captions are in one of the demeaning word lists. This is done in three ways:
    Lower bound: if every synset the word belongs to is in on of the demeaning word lists.
    Estimate: if the wordâ€™s most common synset is in one of the demeaning word lists.
    Upper bound: if any synset the word belongs to is in one of the demeaning word lists.

    :param model_name:
    :param dataset_name:
    :return:
    """

    # Find if every synset the word belongs to is demeaning word lists
    words_with_all_synsets = words.explode('all_synsets').reset_index(drop=True)
    words_with_all_synsets['is_unsafe'] = words_with_all_synsets['all_synsets'].isin(unsafe_synsets)
    lower_bound = words_with_all_synsets.groupby(['image_id', 'word']).agg({'is_unsafe': 'all'}).reset_index()
    lower_bound['unsafe_word'] = np.where(lower_bound['is_unsafe'], lower_bound['word'], None)
    lower_bound = lower_bound.groupby(['image_id']).agg({'is_unsafe': 'any',
                                                         'unsafe_word': lambda x: ', '.join(
                                                             [w for w in x if w is not None])}).reset_index()
    lower_bound = lower_bound.rename(columns={'is_unsafe': 'is_unsafe_lower',
                                              'unsafe_word': 'unsafe_word_lower'})

    # Find if the word's most common synset is in one of the demeaning word lists
    words['is_unsafe'] = words['most_common_synset'].isin(unsafe_synsets)
    words['unsafe_word'] = np.where(words['is_unsafe'], words['word'], None)
    point = words.groupby(['image_id']).agg({'is_unsafe': 'any',
                                             'unsafe_word': lambda x: ', '.join(
                                                 [w for w in x if w is not None])}).reset_index()
    point = point.rename(columns={'is_unsafe': 'is_unsafe_point',
                                  'unsafe_word': 'unsafe_word_point'})

    # Find if any synset the word belongs to is in one of the demeaning word lists
    words_with_all_synsets['unsafe_word'] = np.where(words_with_all_synsets['is_unsafe'],
                                                     words_with_all_synsets['word'], None)
    upper_bound = words_with_all_synsets.groupby(['image_id']).agg({'is_unsafe': 'any',
                                                                    'unsafe_word': lambda x: ', '.join(
                                                                        [w for w in x if w is not None])}).reset_index()
    upper_bound = upper_bound.rename(columns={'is_unsafe': 'is_unsafe_upper',
                                              'unsafe_word': 'unsafe_word_upper'})

    # Combine lower bound, point, and upper bound
    captions = pd.merge(captions, lower_bound, on=['image_id'], how='left')
    captions = pd.merge(captions, point, on=['image_id'], how='left')
    captions = pd.merge(captions, upper_bound, on=['image_id'], how='left')

    return captions


if __name__ == '__main__':
    unsafe_synsets = load_unsafe_synsets()

    # Iterate through captioners
    for model_name in ['clipcap_coco_mlp', 'clipcap_coco_transformer', 'clipcap_conceptual_mlp']:

        for dataset_name in ['coco']:
            words, captions = load_synsets_in_captions(model_name, dataset_name)
            captions = demeaning_words(words, unsafe_synsets, captions)

            # Save
            captions.to_csv(
                os.path.join(f'processed_data', f'measurements', f'demeaning',f'{model_name}.csv'),
                index=False)
